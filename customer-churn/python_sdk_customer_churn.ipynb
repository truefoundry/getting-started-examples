{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QOAnoPl-dlSY",
    "tags": []
   },
   "source": [
    "# Deploy Machine Learning Job on Truefoundry\n",
    "This notebook demonstrates a demo on how you can deploy a classification model trained on customer churn dataset as well as log the job metadata on truefoundry platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before we begin, make sure you have the following prerequisites in place:\n",
    "\n",
    "1. **Install `servicefoundry`** (Note: `servicefoundry` is pre-installed in Truefoundry notebooks). You can install it using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U \"servicefoundry\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Login to servicefoundry**\n",
    "\n",
    "Enter your host in the `--host` argument, eg: \"https://your-domain.truefoundry.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sfy login --host \"<ENTER YOUR HOST HERE>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2c6nhZIxSvl2"
   },
   "source": [
    "3. **Select the `Workspace`** in which you want to deploy your application. <br>Once you run the cell below you will get a prompt to enter your workspace. <br>\n",
    "    * **Step 1:** Navigate to the **Workspace** tab on the left panel of your User Interface.\n",
    "    * **Step 2:** Identify the Workspace you want to deploy the application in.\n",
    "    * **Step 3:** Copy the Workspace FQN <br>\n",
    "    ![Copying Workspace FQN](https://files.readme.io/730fee2-Screenshot_2023-02-28_at_2.08.34_PM.png)\n",
    "    * **Step 4:** Paste the  Workspace FQN in the prompt and press enter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "elr9RXA4En1G"
   },
   "outputs": [],
   "source": [
    "workspace_fqn = input(\"Enter your Workspace FQN: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Setup Logging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clone the Getting Started Repo\n",
    "\n",
    "In this step, we will clone the Truefoundry Getting Started repository. This repository contains the job code that we are going to deploy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/truefoundry/getting-started-examples.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's `cd` into the directory containing our inference code, i.e `getting-started-examples/customer-churn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd getting-started-examples/customer-churn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9K_qE3ol5De",
    "tags": []
   },
   "source": [
    "## Code Structure\n",
    "\n",
    "Before we proceed, let's take a quick look at the structure of the code you'll be deploying:\n",
    "\n",
    "```text\n",
    ".\n",
    "|_ main.py : Contains the training code\n",
    "|_ requirements.txt : Dependency file\n",
    "```\n",
    "\n",
    "Let's help you understand the key elements in the main.py code that you'll be deploying:\n",
    "\n",
    "- **Hyperparameters and argparse:**  \n",
    "  Firstly, the argparse library is used to handle hyperparameters as command-line arguments. This dynamic approach allows altering hyperparameters without modifying the code itself. These command-line hyperparameters are then passed to the train_model function.\n",
    "- **`train_model` Function:**  \n",
    "  The train_model function is responsible for training the K-Nearest Neighbors (KNN) classifier using the provided hyperparameters. It also calculates the metrics for evaluating the model. Then it passes all of this info to `experiment_track` function\n",
    "- **`experiment_track` Function:**  \n",
    "  The experiment_track function logs experiment-related details into the ML Repo. Specifically:\n",
    "  - It Initializes the mlfoundry client.\n",
    "  - Creates an ML Repo named \"churn-pred.\"\n",
    "  - Creates a run within the ML Repo to track this experiment.\n",
    "  - Logs hyperparameters and metrics.\n",
    "  - Logs the trained model using the log_model method, enabling deployment via Model Deployment.\n",
    "\n",
    "```python main.py\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier as Classification\n",
    "import mlfoundry as mlf\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def experiment_track(model, params, metrics):\n",
    "    # initialize the mlfoundry client.\n",
    "    mlf_api = mlf.get_client()\n",
    "\n",
    "    # create a ml repo\n",
    "    mlf_api.create_ml_repo(\"churn-pred\")\n",
    "    # create a run\n",
    "    mlf_run = mlf_api.create_run(\n",
    "        ml_repo=\"churn-pred\", run_name=\"churn-train-job\"\n",
    "    )\n",
    "    # log the hyperparameters\n",
    "    mlf_run.log_params(params)\n",
    "    # log the metrics\n",
    "    mlf_run.log_metrics(metrics)\n",
    "    # log the model\n",
    "    model_version = mlf_run.log_model(\n",
    "        name=\"churn-model\",\n",
    "        model=model,\n",
    "        # specify the framework used (in this case sklearn)\n",
    "        framework=mlf.ModelFramework.SKLEARN,\n",
    "        description=\"churn-prediction-model\",\n",
    "    )\n",
    "    # return the model's fqn\n",
    "    return model_version.fqn\n",
    "\n",
    "\n",
    "def train_model(hyperparams):\n",
    "\n",
    "    df = pd.read_csv(\"https://raw.githubusercontent.com/nikp1172/datasets-sample/main/Churn_Modelling.csv\")\n",
    "    X = df.iloc[:, 3:-1].drop([\"Geography\", \"Gender\"], axis=1)\n",
    "    y = df.iloc[:, -1]\n",
    "    # Create train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Initialize the KNN Classifier\n",
    "    classifier = Classification(\n",
    "        n_neighbors=hyperparams['n_neighbors'],\n",
    "        weights=hyperparams['weights'],\n",
    "    )\n",
    "\n",
    "    # Fit the classifier with the training data\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "\n",
    "    # Get the metrics\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"f1_score\": f1_score(y_test, y_pred, average=\"weighted\"),\n",
    "        \"precision\": precision_score(y_test, y_pred, average=\"weighted\"),\n",
    "        \"recall\": recall_score(y_test, y_pred, average=\"weighted\"),\n",
    "    }\n",
    "\n",
    "    # Log the experiment\n",
    "    experiment_track(classifier, classifier.get_params(), metrics)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "\n",
    "    # Setup the argument parser by instantiating `ArgumentParser` class\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # Add the hyperparameters as arguments\n",
    "    parser.add_argument(\n",
    "        \"--n_neighbors\",\n",
    "        type=int,\n",
    "        required=True,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--weights\",\n",
    "        type=str,\n",
    "        required=True,\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    hyperparams = vars(args)\n",
    "\n",
    "    # Train the model\n",
    "    train_model(hyperparams)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TR7Ped05Crli"
   },
   "source": [
    "## Deploying Your Machine Learning Job\n",
    "\n",
    "Now, let's move on to the deployment steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Set Up Deployment Configuration\n",
    "In this step, you will define your deployment configuration using the Truefoundry Python SDK. We will provide explanations for each parameter and guide you through the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Name\n",
    "In the provided Python script, set a unique identifier for your job using the name field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"churn-prediction-job\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image\n",
    "\n",
    "* Choosing the Right Approach for specifying image:\n",
    "    Depending on your scenario, you can choose to deploy either a pre-built Docker image or build a Docker image from your source code.\n",
    "    \n",
    "* Using Pre-Built Images\n",
    "    If you already have a Docker image that you've previously built and pushed to a container registry, you can use the `Image` class.\n",
    "    The `Image class` would simply reference the pre-built image URL and use it for deployment.\n",
    "* Using Build for Source Code\n",
    "    In cases where you don't have a pre-built image, you'll use the `Build` option to create an image from your source code.\n",
    "    This scenario applies when you want to package and deploy your application from scratch.\n",
    "    * Creating DockerFile with PythonBuild\n",
    "        If you don't have a Dockerfile but your application is written in Python, you can use the `PythonBuild` class.\n",
    "        The `PythonBuild` class will inspect your Python code and create a Dockerfile automatically based on the code's requirements.\n",
    "    * Choosing DockerBuild for Dockerfile\n",
    "        If you have a pre-existing Dockerfile, you can use the `DockerBuild` class.\n",
    "        This allows you to directly reference the Dockerfile present in your code repository.\n",
    "\n",
    "In this case given we did not have a prebuilt image, and no dockerfile in our source code we are using PythonBuild, which takes our code configuration from us and templatizes a Dockerfile for us.\n",
    "\n",
    "\n",
    "In the Command field, enter the command to execute your training job, including placeholders for hyperparameters like {{n_neighbors}}, {{weights}}, etc.  \n",
    "These are going to be the same names we specify in the Params configuration below, so keep this in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from servicefoundry import Build, PythonBuild, LocalSource\n",
    "\n",
    "image = Build(\n",
    "    build_spec=PythonBuild(\n",
    "        command=\"python main.py --n_neighbors {{n_neighbors}} --weights {{weights}}\",\n",
    "        requirements_path=\"requirements.txt\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Params\n",
    "The `Param` option empowers you to configure hyperparameters and pass them to create distinct job runs.\n",
    "\n",
    "For each parameter, provide the following details:\n",
    "\n",
    "- **Name:** Enter a descriptive name for the parameter.\n",
    "- **Default value:** Specify the default value for the parameter.\n",
    "- **Description:** Include a brief description of the parameter's purpose.\n",
    "- **Param type:** Can be either string or an ML Repo\n",
    "\n",
    "Note that the name of Param are same as what we filled in the comman's {{}} template. `python main.py --n_neighbors {{n_neighbors}} --weights {{weights}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from servicefoundry import Param\n",
    "\n",
    "params = [\n",
    "    Param(\n",
    "        name=\"n_neighbors\",\n",
    "        default=5,\n",
    "        description=\"Number of neighbors to use by default\"\n",
    "    ),\n",
    "    Param(\n",
    "        name=\"weights\",\n",
    "        default=\"uniform\",\n",
    "        description=\"Weight function used in prediction.  Possible values: uniform, distance\"\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resources\n",
    "Allocate computing resources (CPU, memory, storage) for your service using the Resources option.<br>\n",
    "* **CPU** refers to the computing power available to your application\n",
    "* **Memory** refers to how much space your application has to hold and work with data while it's running\n",
    "* **Ephemeral storage** is where your application can temporarily store files and data\n",
    "\n",
    "Requests and Limits:\n",
    "\n",
    "* **Request** is like asking for a certain amount of a resource. It's what your application initially asks for to start working properly.\n",
    "* **Limit** is like setting a maximum value. It restricts how much of a resource (like CPU or memory) your application can use.\n",
    "\n",
    "So for each category of resource you specify the Request and Limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from servicefoundry import Resources\n",
    "\n",
    "resources = Resources(\n",
    "    memory_limit=500,\n",
    "    memory_request=500,\n",
    "    ephemeral_storage_limit=600,\n",
    "    ephemeral_storage_request=600,\n",
    "    cpu_limit=0.3,\n",
    "    cpu_request=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Bring all of the configuration together via the Job Class and Deploy\n",
    "\n",
    "To deploy your machine learning job, you need to create an instance of the `Job` class provided by the servicefoundry library. This instance will encapsulate all the necessary configurations and parameters for deploying and managing your job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from servicefoundry import Job\n",
    "\n",
    "job = Job(\n",
    "    name=name,\n",
    "    image=image,\n",
    "    resources=resources,\n",
    "    params=params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "After configuring your deployment settings, you can deploy the job using the deploy method. Here we are replacing the WORKSPACE_FQN with the workspace_fqn we stored earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the job\n",
    "job.deploy(workspace_fqn=workspace_fqn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the build is complete, you will see a link to the dashboard after a message like `You can find the application on the dashboard:-`. <br>Click on the link to access the deployment dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effortless Hyperparameter Experimentation\n",
    "\n",
    "Once your deployment is active, navigate to your specific job by clicking on it. This action will open a dedicated dashboard displaying various job details, including the **Run Job** button.\n",
    "\n",
    "![](https://files.readme.io/cfff7cd-Screenshot_2023-08-23_at_1.48.02_PM.png)\n",
    "\n",
    "Clicking this button will trigger a modal to appear:\n",
    "\n",
    "![](https://files.readme.io/971a7fe-Screenshot_2023-08-23_at_1.51.38_PM.png)\n",
    "\n",
    "Within this modal, you can effortlessly adjust hyperparameter values for rapid experimentation.\n",
    "\n",
    "After configuring the modal, submit it using the Run Job button. This action will redirect you to the Job Runs tab. Within a few moments, your job status should switch to Finished.\n",
    "\n",
    "Proceed by clicking on the logs button to access your job's results:\n",
    "\n",
    "![](https://files.readme.io/1b79056-Screenshot_2023-08-28_at_7.17.03_AM.png)\n",
    "\n",
    "Now closing, clicking the purple **churn-train-job** badge will grant you access to the Key Metrics, Hyperparameters, Logged Model, and Associated Artifacts from the run.\n",
    "\n",
    "\n",
    "![](https://files.readme.io/0113700-Screenshot_2023-08-28_at_7.14.36_AM.png)\n",
    "\n",
    "# Additional Capabilities of Jobs\n",
    "\n",
    "Let's delve into the advanced functionalities that Jobs offer, extending beyond deployment strategies:\n",
    "\n",
    "- **Continuous Integration/Continuous Deployment (CI/CD) via Truefoundry:** Integrate Jobs with Truefoundry for streamlined CI/CD pipelines, ensuring efficient code integration, testing, and deployment.\n",
    "- **Cron Jobs:** Schedule Jobs to run at specified intervals using cron-like expressions, automating recurring tasks and processes.\n",
    "- **Job Parametrization:** Configure Jobs with parameters, allowing you to customize execution by providing dynamic input values.\n",
    "- **Programmatic Job Triggers:** Trigger Jobs programmatically via APIs, enabling seamless automation and integration with external systems.\n",
    "- **Additional Configurations:** Access a range of supplementary configurations to fine-tune job behavior and optimize performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".conda-jupyter-base",
   "language": "python",
   "name": "conda-env-.conda-jupyter-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
