{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QOAnoPl-dlSY",
    "tags": []
   },
   "source": [
    "# Deploy Machine Learning Job on Truefoundry\n",
    "This notebook demonstrates a demo on how you can deploy an image classification model trained on mnist dataset as a Gradio App on truefoundry platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before we begin, make sure you have the following prerequisites in place:\n",
    "\n",
    "1. **Install `servicefoundry`** (Note: `servicefoundry` is pre-installed in Truefoundry notebooks). You can install it using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U \"servicefoundry\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Login to servicefoundry**\n",
    "\n",
    "Enter your host in the `--host` argument, eg: \"https://your-domain.truefoundry.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sfy login --host \"<ENTER YOUR HOST HERE>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2c6nhZIxSvl2"
   },
   "source": [
    "3. **Select the `Workspace`** in which you want to deploy your application. <br>Once you run the cell below you will get a prompt to enter your workspace. <br>\n",
    "    * **Step 1:** Navigate to the **Workspace** tab on the left panel of your User Interface.\n",
    "    * **Step 2:** Identify the Workspace you want to deploy the application in.\n",
    "    * **Step 3:** Copy the Workspace FQN <br>\n",
    "    ![Copying Workspace FQN](https://files.readme.io/730fee2-Screenshot_2023-02-28_at_2.08.34_PM.png)\n",
    "    * **Step 4:** Paste the  Workspace FQN in the prompt and press enter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "elr9RXA4En1G"
   },
   "outputs": [],
   "source": [
    "workspace_fqn = input(\"Enter your Workspace FQN: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Setup Logging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clone the Getting Started Repo\n",
    "\n",
    "In this step, we will clone the Truefoundry Getting Started repository. This repository contains the job code that we are going to deploy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/truefoundry/getting-started-examples.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's `cd` into the directory containing our inference code, i.e `getting-started-examples/customer-churn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd getting-started-examples/customer-churn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9K_qE3ol5De",
    "tags": []
   },
   "source": [
    "### Code Structure\n",
    "Before we proceed, let's take a quick look at the structure of the code you'll be deploying:\n",
    "\n",
    "```\n",
    ".\n",
    "|_ app.py: Contains the Gradio Service code used to serve your model.\n",
    "|_ requirements.txt: Dependency file.\n",
    "|_ gen_example_images.py: Code to generate example images that you can use to test your Gradio service.\n",
    "|_ train.py: Contains the Training code used to train the model we are deploying.\n",
    "```\n",
    "\n",
    "The `app.py` file houses the code that enables you to deploy and interact with your trained model using Gradio. Here's a brief overview of its components:\n",
    "\n",
    "* **Prediction Function:**\n",
    "    * The predict function is responsible for making predictions using the trained model.\n",
    "    * Preprocess the input image and prepare it for model inference.\n",
    "    * Load the trained model and perform predictions to identify the digit in the image.\n",
    "* **Gradio Interface Setup:**\n",
    "    * Create a Gradio interface that takes an input image and produces a predicted label.\n",
    "    * Provide examples of images (\"0.jpg\" and \"1.jpg\") for users to easily test the interface.\n",
    "\n",
    "```python\n",
    "import gradio as gr  \n",
    "import tensorflow as tf  \n",
    "import numpy as np  \n",
    "from PIL import Image\n",
    "\n",
    "def predict(img_arr):\n",
    "\t# Preprocess the image before passing it to the model\n",
    "  img_arr = tf.expand_dims(img_arr, 0)  \n",
    "  img_arr = img_arr[:, :, :, 0]  # Keep only the first channel (grayscale)\n",
    "\n",
    "\t# Load the trained model\n",
    "  loaded_model = tf.keras.models.load_model('mnist_model.h5')\n",
    "\n",
    "\t# Make predictions\n",
    "  predictions = loaded_model.predict(img_arr)  \n",
    "  predicted_label = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "  return str(predicted_label)\n",
    "\n",
    "# Setup the gradio interface\n",
    "gr.Interface(fn=predict,  \n",
    "             inputs=\"image\",  \n",
    "             outputs=\"label\",  \n",
    "             examples=[[\"0.jpg\"], [\"1.jpg\"]]  \n",
    ").launch(server_name=\"0.0.0.0\", server_port=8080)\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TR7Ped05Crli"
   },
   "source": [
    "## Deploying Your Machine Learning Job\n",
    "\n",
    "Now, let's move on to the deployment steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Set Up Deployment Configuration\n",
    "In this step, you will define your deployment configuration using the ServiceFoundry CLI. We will provide explanations for each configuration, and afterwards we will bring all of this together into a `servicefoundry.yaml` file.<br>\n",
    "\n",
    "For deploying via ServiceFoundry CLI you need to create a `servicefoundry.yaml` file with a YAML configuration of your Deployment\n",
    "\n",
    "#### Name\n",
    "In the provided Python script, set a unique identifier for your job using the name field.\n",
    "\n",
    "```yaml\n",
    "name: churn-prediction-job\n",
    "```\n",
    "\n",
    "#### Image\n",
    "\n",
    "* Choosing the Right Approach for specifying image:\n",
    "    Depending on your scenario, you can choose to deploy either a pre-built Docker image or build a Docker image from your source code.\n",
    "    \n",
    "* Using Pre-Built Images\n",
    "    If you already have a Docker image that you've previously built and pushed to a container registry, you can use `type: image`.\n",
    "    The `type: image` would simply reference the pre-built image URL and use it for deployment.\n",
    "* Using Build for Source Code\n",
    "    In cases where you don't have a pre-built image, you'll use the Build option to create an image from your source code.\n",
    "    This scenario applies when you want to package and deploy your application from scratch.\n",
    "    * Creating DockerFile with PythonBuild\n",
    "        If you don't have a Dockerfile but your application is written in Python, you can use the `type: tfy-python-buildpack`.\n",
    "        The PythonBuild class will inspect your Python code and create a Dockerfile automatically based on the code's requirements.\n",
    "    * Choosing DockerBuild for Dockerfile\n",
    "        If you have a pre-existing Dockerfile, you can use the `type: dockerfile`.\n",
    "        This allows you to directly reference the Dockerfile present in your code repository.\n",
    "\n",
    "In this case given we did not have a prebuilt image, and no dockerfile in our source code we are using `tfy-python-buildpack`, which takes our code configuration from us and templatizes a Dockerfile for us.\n",
    "\n",
    "```yaml\n",
    "image:\n",
    "  type: build\n",
    "  build_spec:\n",
    "    type: tfy-python-buildpack\n",
    "    command: python main.py --n_neighbors {{n_neighbors}} --weights {{weights}}\n",
    "    python_version: '3.9'\n",
    "    requirements_path: requirements.txt\n",
    "    build_context_path: ./\n",
    "  build_source:\n",
    "    type: local\n",
    "```\n",
    "\n",
    "#### Params\n",
    "The `params:` key empowers you to configure hyperparameters and pass them to create distinct job runs.\n",
    "\n",
    "For each parameter, provide the following details:\n",
    "\n",
    "- **Name:** Enter a descriptive name for the parameter.\n",
    "- **Default value:** Specify the default value for the parameter.\n",
    "- **Description:** Include a brief description of the parameter's purpose.\n",
    "- **Param type:** Can be either string or an ML Repo\n",
    "\n",
    "Note that the name of Param are same as what we filled in the comman's {{}} template. `python main.py --n_neighbors {{n_neighbors}} --weights {{weights}`\n",
    "\n",
    "```yaml\n",
    "params:\n",
    "  - name: n_neighbors\n",
    "    default: '5'\n",
    "    param_type: string\n",
    "    description: 'Number of neighbors to use by default'\n",
    "  - name: weights\n",
    "    default: uniform\n",
    "    param_type: string\n",
    "    description: 'Weight function used in prediction. Possible values: uniform, distance'\n",
    "```\n",
    "\n",
    "#### Resources\n",
    "Allocate computing resources (CPU, memory, storage) for your job using the Resources option.<br>\n",
    "* **CPU** refers to the computing power available to your application\n",
    "* **Memory** refers to how much space your application has to hold and work with data while it's running\n",
    "* **Ephemeral storage** is where your application can temporarily store files and data\n",
    "\n",
    "Requests and Limits:\n",
    "\n",
    "* **Request** is like asking for a certain amount of a resource. It's what your application initially asks for to start working properly.\n",
    "* **Limit** is like setting a maximum value. It restricts how much of a resource (like CPU or memory) your application can use.\n",
    "\n",
    "So for each category of resource you specify the Request and Limits\n",
    "\n",
    "```yaml\n",
    "resources:\n",
    "  cpu_limit: 0.3\n",
    "  gpu_count: 0\n",
    "  cpu_request: 0.3\n",
    "  memory_limit: 500\n",
    "  memory_request: 500\n",
    "  ephemeral_storage_limit: 600\n",
    "  ephemeral_storage_request: 600\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Bring all of the configuration together via the Job Class and Deploy\n",
    "\n",
    "To deploy your machine learning job, you need to create a `servicefoundry.yaml` file with the configuration library. This will encapsulate all the necessary configurations and parameters for deploying and managing your job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile servicefoundry.yaml\n",
    "name: churn-prediction-job\n",
    "type: job\n",
    "image:\n",
    "  type: build\n",
    "  build_spec:\n",
    "    type: tfy-python-buildpack\n",
    "    command: python main.py --n_neighbors {{n_neighbors}} --weights {{weights}}\n",
    "    python_version: '3.9'\n",
    "    requirements_path: requirements.txt\n",
    "    build_context_path: ./\n",
    "  build_source:\n",
    "    type: local\n",
    "params:\n",
    "  - name: n_neighbors\n",
    "    default: '5'\n",
    "    param_type: string\n",
    "    description: Number of neighbors to use by default\n",
    "  - name: weights\n",
    "    default: uniform\n",
    "    param_type: string\n",
    "    description: 'Weight function used in prediction. Possible values: uniform, distance'\n",
    "resources:\n",
    "  cpu_limit: 0.3\n",
    "  cpu_request: 0.3\n",
    "  memory_limit: 500\n",
    "  memory_request: 500\n",
    "  ephemeral_storage_limit: 600\n",
    "  ephemeral_storage_request: 600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "After configuring your deployment settings, you can deploy the job using the deploy method. Here we are replacing the WORKSPACE_FQN with the workspace_fqn we stored earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the job\n",
    "!servicefoundry deploy --workspace-fqn=YOUR_WORKSPACE_FQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the build is complete, you will see a link to the dashboard after a message like `You can find the application on the dashboard:-`. <br>Click on the link to access the deployment dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effortless Hyperparameter Experimentation\n",
    "\n",
    "Once your deployment is active, navigate to your specific job by clicking on it. This action will open a dedicated dashboard displaying various job details, including the **Run Job** button.\n",
    "\n",
    "![](https://files.readme.io/cfff7cd-Screenshot_2023-08-23_at_1.48.02_PM.png)\n",
    "\n",
    "Clicking this button will trigger a modal to appear:\n",
    "\n",
    "![](https://files.readme.io/971a7fe-Screenshot_2023-08-23_at_1.51.38_PM.png)\n",
    "\n",
    "Within this modal, you can effortlessly adjust hyperparameter values for rapid experimentation.\n",
    "\n",
    "After configuring the modal, submit it using the Run Job button. This action will redirect you to the Job Runs tab. Within a few moments, your job status should switch to Finished.\n",
    "\n",
    "Proceed by clicking on the logs button to access your job's results:\n",
    "\n",
    "![](https://files.readme.io/1b79056-Screenshot_2023-08-28_at_7.17.03_AM.png)\n",
    "\n",
    "Now closing, clicking the purple **churn-train-job** badge will grant you access to the Key Metrics, Hyperparameters, Logged Model, and Associated Artifacts from the run.\n",
    "\n",
    "\n",
    "![](https://files.readme.io/0113700-Screenshot_2023-08-28_at_7.14.36_AM.png)\n",
    "\n",
    "# Additional Capabilities of Jobs\n",
    "\n",
    "Let's delve into the advanced functionalities that Jobs offer, extending beyond deployment strategies:\n",
    "\n",
    "- **Continuous Integration/Continuous Deployment (CI/CD) via Truefoundry:** Integrate Jobs with Truefoundry for streamlined CI/CD pipelines, ensuring efficient code integration, testing, and deployment.\n",
    "- **Cron Jobs:** Schedule Jobs to run at specified intervals using cron-like expressions, automating recurring tasks and processes.\n",
    "- **Job Parametrization:** Configure Jobs with parameters, allowing you to customize execution by providing dynamic input values.\n",
    "- **Programmatic Job Triggers:** Trigger Jobs programmatically via APIs, enabling seamless automation and integration with external systems.\n",
    "- **Additional Configurations:** Access a range of supplementary configurations to fine-tune job behavior and optimize performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".conda-jupyter-base",
   "language": "python",
   "name": "conda-env-.conda-jupyter-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
