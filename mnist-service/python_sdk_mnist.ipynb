{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QOAnoPl-dlSY",
    "tags": []
   },
   "source": [
    "# Deploy Machine Learning Service on Truefoundry\n",
    "This notebook demonstrates a demo on how you can deploy an image classification model trained on mnist dataset as a Gradio App on truefoundry platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before we begin, make sure you have the following prerequisites in place:\n",
    "\n",
    "1. **Install `servicefoundry`** (Note: `servicefoundry` is pre-installed in Truefoundry notebooks). You can install it using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U \"servicefoundry\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Login to servicefoundry**\n",
    "\n",
    "Enter your host in the `--host` argument, eg: \"https://your-domain.truefoundry.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sfy login --host \"<ENTER YOUR HOST HERE>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2c6nhZIxSvl2"
   },
   "source": [
    "3. **Select the `Workspace`** in which you want to deploy your application. <br>Once you run the cell below you will get a prompt to enter your workspace. <br>\n",
    "    * **Step 1:** Navigate to the **Workspace** tab on the left panel of your User Interface.\n",
    "    * **Step 2:** Identify the Workspace you want to deploy the application in.\n",
    "    * **Step 3:** Copy the Workspace FQN <br>\n",
    "    ![Copying Workspace FQN](https://files.readme.io/730fee2-Screenshot_2023-02-28_at_2.08.34_PM.png)\n",
    "    * **Step 4:** Paste the  Workspace FQN in the prompt and press enter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "elr9RXA4En1G"
   },
   "outputs": [],
   "source": [
    "workspace_fqn = input(\"Enter your Workspace FQN: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Setup Logging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clone the Getting Started Repo\n",
    "\n",
    "In this step, we will clone the Truefoundry Getting Started repository. This repository contains the service code that we are going to deploy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/truefoundry/getting-started-examples.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's `cd` into the directory containing our inference code, i.e `getting-started-examples/deploy-ml-model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd getting-started-examples/mnist-service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9K_qE3ol5De",
    "tags": []
   },
   "source": [
    "### Code Structure\n",
    "Before we proceed, let's take a quick look at the structure of the code you'll be deploying:\n",
    "\n",
    "```\n",
    ".\n",
    "|_ app.py: Contains the Gradio Service code used to serve your model.\n",
    "|_ requirements.txt: Dependency file.\n",
    "|_ gen_example_images.py: Code to generate example images that you can use to test your Gradio service.\n",
    "|_ train.py: Contains the Training code used to train the model we are deploying.\n",
    "```\n",
    "\n",
    "The `app.py` file houses the code that enables you to deploy and interact with your trained model using Gradio. Here's a brief overview of its components:\n",
    "\n",
    "* **Prediction Function:**\n",
    "    * The predict function is responsible for making predictions using the trained model.\n",
    "    * Preprocess the input image and prepare it for model inference.\n",
    "    * Load the trained model and perform predictions to identify the digit in the image.\n",
    "* **Gradio Interface Setup:**\n",
    "    * Create a Gradio interface that takes an input image and produces a predicted label.\n",
    "    * Provide examples of images (\"0.jpg\" and \"1.jpg\") for users to easily test the interface.\n",
    "\n",
    "```python\n",
    "import gradio as gr  \n",
    "import tensorflow as tf  \n",
    "import numpy as np  \n",
    "from PIL import Image\n",
    "\n",
    "def predict(img_arr):\n",
    "\t# Preprocess the image before passing it to the model\n",
    "  img_arr = tf.expand_dims(img_arr, 0)  \n",
    "  img_arr = img_arr[:, :, :, 0]  # Keep only the first channel (grayscale)\n",
    "\n",
    "\t# Load the trained model\n",
    "  loaded_model = tf.keras.models.load_model('mnist_model.h5')\n",
    "\n",
    "\t# Make predictions\n",
    "  predictions = loaded_model.predict(img_arr)  \n",
    "  predicted_label = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "  return str(predicted_label)\n",
    "\n",
    "# Setup the gradio interface\n",
    "gr.Interface(fn=predict,  \n",
    "             inputs=\"image\",  \n",
    "             outputs=\"label\",  \n",
    "             examples=[[\"0.jpg\"], [\"1.jpg\"]]  \n",
    ").launch(server_name=\"0.0.0.0\", server_port=8080)\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TR7Ped05Crli"
   },
   "source": [
    "## Deploying Your Machine Learning Service\n",
    "\n",
    "Now, let's move on to the deployment steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Set Up Deployment Configuration\n",
    "In this step, you will define your deployment configuration using the Truefoundry Python SDK. We will provide explanations for each parameter and guide you through the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Name\n",
    "In the provided Python script, set a unique identifier for your service using the name field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"mnist-service\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Source\n",
    "Choose whether to deploy a pre-built Docker image or build a Docker image from source code using the Build options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from servicefoundry import Build, PythonBuild, LocalSource\n",
    "\n",
    "image = Build(\n",
    "    build_spec=PythonBuild(\n",
    "        command=\"python app.py\",\n",
    "        requirements_path=\"requirements.txt\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ports\n",
    "Specify the port for routing customer traffic to your deployed application using the Port option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from servicefoundry import Port\n",
    "\n",
    "ports = [\n",
    "    Port(\n",
    "        port=8080,\n",
    "        host=\"mnist-service-docs-ws-8080.tfy-gcp-standard-usce1.devtest.truefoundry.tech\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resources\n",
    "Allocate computing resources (CPU, memory, storage) for your service using the Resources option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from servicefoundry import Resources\n",
    "\n",
    "resources = Resources(\n",
    "    memory_limit=500,\n",
    "    memory_request=500,\n",
    "    ephemeral_storage_limit=600,\n",
    "    ephemeral_storage_request=600,\n",
    "    cpu_limit=0.3,\n",
    "    cpu_request=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Bring all of the configuration together via the Service Class and Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from servicefoundry import Service\n",
    "\n",
    "service = Service(\n",
    "    name=name,\n",
    "    image=image,\n",
    "    ports=ports,\n",
    "    resources=resources,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "After configuring your deployment settings, you can deploy the service using the deploy method. Here we are replacing the WORKSPACE_FQN with the workspace_fqn we stored earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the service\n",
    "service.deploy(workspace_fqn=workspace_fqn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the build is complete, you will see a link to the dashboard after a message like `You can find the application on the dashboard:-`. <br>Click on the link to access the deployment dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting with the Application\n",
    "\n",
    "Clicking the link will open up the dashboard dedicated to your service, where you'll have access to various details.\n",
    "\n",
    "Here you will be able to see the Endpoint of your service at the top right corner. You can click on the Endpoint to open your application.\n",
    "\n",
    "![](https://files.readme.io/142331b-8e96f01-Screenshot_2023-06-30_at_1.54.29_PM.png)\n",
    "\n",
    "Now you can click on one of the Images from the two options and see what predictions your model gives:\n",
    "\n",
    "![](https://files.readme.io/d2f8d05-bba9cc1-Screenshot_2023-06-30_at_1.57.15_PM.png)\n",
    "\n",
    "Congratulations! You have successfully deployed the model using Truefoundry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enabling Autoscaling for your Service\n",
    "\n",
    "In this section, we'll explore enabling autoscaling for your service, a feature that allows your application to dynamically adjust its resources based on real-time demand and predefined metrics. Autoscaling optimizes performance, responsiveness, and resource efficiency.\n",
    "\n",
    "## Scaling with Replicas and Pods\n",
    "\n",
    "In Kubernetes and containerized environments, replicas and pods are essential concepts for managing application availability and scalability.\n",
    "\n",
    "When deploying applications in a Kubernetes cluster, you specify the number of replicas for your service. Each replica is an identical instance of your application within a pod. A pod is the smallest deployable unit in Kubernetes, comprising one or more closely connected containers that share network and storage.\n",
    "\n",
    "By setting the replica count, you control how many pod instances run concurrently, directly affecting your application's traffic handling capacity.\n",
    "\n",
    "### Handling Demand\n",
    "\n",
    "More replicas allocate more pods to manage incoming traffic, distributing the workload and improving responsiveness during spikes. Scaling by adjusting replicas aligns your application's capacity with varying traffic.\n",
    "\n",
    "### Setting the number of replicas\n",
    "\n",
    "To configure the number of replicas your service should have via the PythonSDK, you can add the following to your service deployment configuration:\n",
    "\n",
    "```python\n",
    "...\n",
    "service = Service(\n",
    "    ...\n",
    "    replicas=1,\n",
    "    ...\n",
    ")\n",
    "...\n",
    "```\n",
    "\n",
    "Next, we'll explore how autoscaling improves performance by dynamically adjusting replicas based on real-time metrics and demand.\n",
    "\n",
    "## Autoscaling Overview\n",
    "\n",
    "Autoscaling involves dynamically adjusting computing resources based on real-time demand and predefined metrics. This optimization ensures that your service efficiently utilizes resources while responding to varying traffic loads.\n",
    "\n",
    "### Autoscaling Configuration\n",
    "\n",
    "Autoscaling configuration involves setting minimum and maximum replica counts as well as defining metrics that trigger autoscaling actions. Here are the available settings for autoscaling:\n",
    "\n",
    "- **Minimum Replicas:** The minimum number of replicas to keep available.\n",
    "- **Maximum Replicas:** The maximum number of replicas to keep available.\n",
    "- **Cooldown Period:** The period to wait after the last active trigger before scaling resources back to 0.\n",
    "\n",
    "### Configuring Autoscaling via UI\n",
    "\n",
    "To configure autoscaling parameters for your service via the PythonSDK, you can add the following to your service deployment configuration:\n",
    "\n",
    "```python\n",
    "from servicefoundry import Service, \n",
    "...\n",
    "service = Service(\n",
    "    ...\n",
    "    replicas=Autoscaling(\n",
    "        min_replicas=1,\n",
    "        max_replicas=3,\n",
    "        cooldown_period=30,\n",
    "    ),\n",
    "    ...\n",
    ")\n",
    "...\n",
    "```\n",
    "\n",
    "### Autoscaling Metrics\n",
    "\n",
    "Autoscaling metrics guide the system in dynamically adjusting resource allocation based on real-time conditions. They ensure your service can adapt to changing demands while maintaining optimal performance. We support the following three types of autoscaling metrics:\n",
    "\n",
    "1. **RPSMetric (Requests Per Second Metric):** Monitors the rate of incoming requests measured in requests per second. Suitable for applications with varying request loads over time.\n",
    "\n",
    "2. **CPUUtilizationMetric (CPU Utilization Metric):** Monitors the percentage of CPU resources in use. Ideal for applications where performance correlates with CPU usage.\n",
    "\n",
    "3. **TimeRange:** Allows scheduling autoscaling actions based on specific time periods. Useful for applications with predictable traffic patterns.\n",
    "\n",
    "# Additional Capabilities of Services\n",
    "\n",
    "Let's explore additional functionalities that Services provide, extending beyond deployment strategies:\n",
    "\n",
    "- **Rollouts Management:** Maintain precise control over how new versions of your application are released to users, ensuring seamless transitions and minimal disruption.\n",
    "- **Endpoint Authentication:** Bolster the security of your endpoints by integrating authentication mechanisms, effectively limiting access and safeguarding sensitive data.\n",
    "- **Health Check Monitoring:** Monitor your services' health through comprehensive health checks, guaranteeing their operational readiness to handle incoming requests.\n",
    "- **Efficient Communication with gRPC:** Leverage the power of gRPC, a high-performance communication protocol, to establish efficient and reliable connections between microservices.\n",
    "- **TensorFlow Serving with gRPC:** Harness the capabilities of TensorFlow Serving in conjunction with gRPC to facilitate machine learning model deployment and communication.\n",
    "- **Intercept Management:** Implement interceptors to exert fine-grained control over network communication, enhancing security measures and facilitating robust logging.\n",
    "- **Scaling Deep Dive:** Gain in-depth insights into the nuances of scaling your services, optimizing resource allocation strategies to seamlessly adapt to varying demands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".conda-jupyter-base",
   "language": "python",
   "name": "conda-env-.conda-jupyter-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
